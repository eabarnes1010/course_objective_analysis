{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-igGXP1oFzvl"
      },
      "source": [
        "# Using a random forest to classify Palmer Penguins üêß by species\n",
        "\n",
        "[Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/) is a dataset consisting of penguin measurements from the Palmer Archipelago in Antarctica. This notebook demonstrates the process to classify this data to species ([Adelie](https://www.antarctica.gov.au/about-antarctica/animals/penguins/adelie-penguin/), [Chinstrap](https://www.antarctica.gov.au/about-antarctica/animals/penguins/chinstrap-penguin/), or [Gentoo](https://www.antarctica.gov.au/about-antarctica/animals/penguins/gentoo-penguin/)) based on bill length, bill depth, flipper length, and body mass using a random forest model.\n",
        "\n",
        "[![Latest release](https://badgen.net/github/release/Naereen/Strapdown.js)](https://github.com/eabarnes1010/course_objective_analysis/tree/main/code)\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eabarnes1010/course_objective_analysis/blob/main/code/random_forest_palmerpenguins.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iqNhQxY9fpTX"
      },
      "source": [
        "\n",
        "*    Example by [Daniel Hueholt](https://hueholt.earth), PhD student at Colorado State University (CSU): [see his original notebook here](https://github.com/dmhuehol/palmerpenguins-classifiers/blob/main/rf_class_palmerpenguins.ipynb)\n",
        "*    Borrows format and some functions from the [CSU Machine Learning Tutorial](https://zenodo.org/record/6686879) including code originally by Prof. Elizabeth Barnes, Zaibeth Carlo Frontera, Aaron Hill, Daniel Hueholt, Wei-Ting Hsiao, and Jamin Rader\n",
        "*    For details on Palmer Penguins and cute penguin art, see the official [Palmer Penguins page at GitHub](https://allisonhorst.github.io/palmerpenguins/) or [Horst et al. 2022](doi.org/10.32614/RJ-2022-020)\n",
        "\n",
        "* Further adapted by: Prof. Elizabeth Barnes for ATS 655 and ATS 780A7 at Colorado State University\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bguVSPBTGDSA"
      },
      "source": [
        "# 0. Set Up Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze7Vq_dxFgvg",
        "outputId": "812fcb06-260c-44b3-fdb3-d786018984d2"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "print('IN_COLAB = ' + str(IN_COLAB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvu20cizGGtC",
        "outputId": "69fb3fc0-1fd2-4fed-d6b7-c8d4e171fc9d"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !pip install palmerpenguins\n",
        "\n",
        "from graphviz import Source # To plot trees\n",
        "import matplotlib.pyplot as plt\n",
        "from palmerpenguins import load_penguins # Dataset\n",
        "import numpy as np\n",
        "import pandas as pd # For confusion matrix\n",
        "import pydot\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.tree import export_graphviz\n",
        "import sys\n",
        "\n",
        "print(f\"python version = {sys.version}\")\n",
        "print(f\"scikit-learn version = {sklearn.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdMYVJEwZ2-S",
        "outputId": "e0c72204-7997-47f9-a4e4-07b5ff9f3437"
      },
      "outputs": [],
      "source": [
        "# Make the path of your own Google Drive accessible to save a figure\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        local_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "    except:\n",
        "        local_path = './'\n",
        "else:\n",
        "    local_path = '../figures/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni2RFOPnLFp4"
      },
      "source": [
        "# 1. Load and look at data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlVkefX7Gz9e",
        "outputId": "1b11af74-adc4-494b-90ee-8d52b15b2a87"
      },
      "outputs": [],
      "source": [
        "X,y = load_penguins(return_X_y=True, drop_na=True) #drop_na removes observations with NaN values\n",
        "\n",
        "print('--- FEATURES ---')\n",
        "print(X.head())\n",
        "feature_names = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
        "\n",
        "print('\\n--- LABELS ---')\n",
        "print(y.head())\n",
        "\n",
        "print('\\n--- UNIQUE LABELS / CLASSES ---')\n",
        "classes = np.unique(y.values) # These are the penguin species\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86CVP59YIACu"
      },
      "source": [
        "We use the features (called `X`) to classify the data by target/label (called `y`). With this data, our targets/labels are the species of each penguin, while our features are physical measurements from each penguin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gocF_A1aJFxH"
      },
      "source": [
        "We need to split the data into **training, validation, and testing datasets**. With Earth science data, we often need to carefully consider the method we use for this in order to prevent \"data leakage\" between these data splits. With penguins, this isn't a concern--so we can just use Scikit's default method to do this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0_hCKvrJoEs"
      },
      "outputs": [],
      "source": [
        "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=100,\n",
        "                                                          random_state=13) # first reserve the held-back testing data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=100,\n",
        "                                                       random_state=13) # then reserve the validation data (for hyperparameter tuning)\n",
        "\n",
        "# Commented lines below are to look at the split data\n",
        "# print('--- TRAINING ---')\n",
        "# print(X_train.head(), y_train.head())\n",
        "# print('--- VALIDATION ---')\n",
        "# print(X_val.head(), y_val.head())\n",
        "# print('--- HELD-BACK TESTING --- ')\n",
        "# print(X_test.head(), y_test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sjeo2vjJ_me"
      },
      "source": [
        "We now have the following variables:\n",
        "\n",
        "\n",
        "*   `X_train` and `y_train`: features and labels used for training\n",
        "*   `X_val` and `y_val`: features and labels used for validation (hyperparameter tuning)\n",
        "*   `X_test` and `y_test`: features and labels held back for testing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWJO5lw7K78f"
      },
      "source": [
        "Now we are ready to set up the random forest model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSgkAGd_LJcn"
      },
      "source": [
        "# 2. Set up and run the random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU-zxH5VLVPC"
      },
      "source": [
        "We define the tunable parameters (hyperparameters) that we'll use for the model. We put these values in a dictionary so they can easily be accessed and modified for future experiments. Note these parameters give good performance because they have already been tuned for this demo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0ZVTfa2KS9n"
      },
      "outputs": [],
      "source": [
        "### MODIFY HYPERPARAMETERS WITHIN THIS CELL\n",
        "fd = {\n",
        "    \"tree_number\": 1,    # number of trees to \"average\" together to create a random forest\n",
        "    \"tree_depth\": 1,   # maximum depth allowed for each tree\n",
        "    \"node_split\": 1,     # minimum number of training samples needed to split a node\n",
        "    \"leaf_samples\": 1,    # minimum number of training samples required to make a leaf node\n",
        "    \"criterion\": 'entropy',  # information gain metric, 'gini' or 'entropy'\n",
        "    \"bootstrap\": False,   # whether to perform \"bagging=bootstrap aggregating\" or not\n",
        "    \"max_samples\": None,  # number of samples to grab when training each tree IF bootstrap=True, otherwise None \n",
        "    \"random_state\": 13,    # set random state for reproducibility\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTu1Q0KqQ4if"
      },
      "source": [
        "We set up the random forest classifier to use the hyperparameters we just specified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOBYEEuAQ9Ld"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(\n",
        "                           n_estimators = fd[\"tree_number\"],\n",
        "                           random_state = fd[\"random_state\"],\n",
        "                           min_samples_split = fd[\"node_split\"],\n",
        "                           min_samples_leaf = fd[\"leaf_samples\"],\n",
        "                           criterion = fd[\"criterion\"],\n",
        "                           max_depth = fd[\"tree_depth\"],\n",
        "                           bootstrap = fd[\"bootstrap\"],\n",
        "                           max_samples = fd[\"max_samples\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoMoVdX2Rce0"
      },
      "source": [
        "Now we are ready to train the random forest to identify the penguins from the measurements in our training dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSB9eLPNRf_t"
      },
      "outputs": [],
      "source": [
        "forest.fit(X_train, y_train) # Runs the forest classifier\n",
        "y_pred = forest.predict(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFgxjs9fVA9R"
      },
      "source": [
        "Let's check how well we did on the training data using the accuracy score and a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "acc = metrics.accuracy_score(y_train, y_pred)\n",
        "print(\"training accuracy: \", np.around(acc*100), '%')\n",
        "\n",
        "cm = confusion_matrix(y_train, y_pred, labels=classes)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9totZLc4VRvv"
      },
      "source": [
        "Now let's try the validation data, to make sure our model applies to data it hasn't seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "p82LOZULUcwj",
        "outputId": "614fc665-664f-446e-fb79-d4fe8efc5353"
      },
      "outputs": [],
      "source": [
        "y_pred_val = forest.predict(X_val)\n",
        "\n",
        "acc = metrics.accuracy_score(y_val, y_pred_val)\n",
        "print(\"validation accuracy: \", np.around(acc*100), '%')\n",
        "confusion_matrix(y_val, y_pred_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmEhbx-kVkWc"
      },
      "source": [
        "Continue tuning the hyperparameters until you're happy with the validation accuracy. Once you are, you can once (and only once!) apply your model on the testing data. Before we do that, it's useful to try to understand how the model makes its decisions. This helps you to check whether it's overfitting, not successfully making decisions, or is ready to roll!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AqAy0fcZhPB"
      },
      "source": [
        "# 3. Explainability and interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKmG9UvKeMPh"
      },
      "source": [
        "## 3.1. Look at an individual tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODLfg4hZX2vk"
      },
      "source": [
        "Random forests are made up of individual decision trees. You can get an idea of how the model is making its decisions by looking at these trees. Simple decision trees are easily *interpretable*, that is, humans can easily understand the method by which the model makes its decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "PQQcmqxMYykh",
        "outputId": "9dcee9ac-72c1-4f68-c3d2-0e6556fe47cc"
      },
      "outputs": [],
      "source": [
        "from graphviz import Source\n",
        "\n",
        "fig_savename = 'rf_penguin_tree'\n",
        "tree_to_plot = 0 # Enter the value of the tree that you want to see!\n",
        "\n",
        "tree = forest[tree_to_plot] # Obtain the tree to plot\n",
        "tree_numstr = str(tree_to_plot) # Adds the tree number to filename\n",
        "\n",
        "complete_savename = fig_savename + '_' + tree_numstr + '.dot'\n",
        "export_graphviz(tree,\n",
        "                out_file=local_path + '/' + complete_savename,\n",
        "                filled=True,\n",
        "                proportion=False,\n",
        "                leaves_parallel=False,\n",
        "                class_names=classes,\n",
        "                feature_names=feature_names)\n",
        "\n",
        "Source.from_file(local_path + complete_savename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N3rDBo1eQA3"
      },
      "source": [
        "## 3.2. Feature importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXdXA759d5aa"
      },
      "source": [
        "Another useful way to understand your random forest is through feature importance, which shows how important each feature was to the random forest's predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "vEdtfAX_eVwS",
        "outputId": "b946bd40-0ff6-42cd-a1d7-71e46ca795da"
      },
      "outputs": [],
      "source": [
        "def calc_importances(rf, feature_list):\n",
        "    ''' Calculate feature importance '''\n",
        "    # Get numerical feature importances\n",
        "    importances = list(rf.feature_importances_)\n",
        "\n",
        "    # List of tuples with variable and importance\n",
        "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
        "\n",
        "    # Sort the feature importances by most important first\n",
        "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "\n",
        "    # Print out the feature and importances \n",
        "    print('')\n",
        "    [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
        "    print('')\n",
        "\n",
        "    return importances\n",
        "\n",
        "def plot_feat_importances(importances, feature_list):\n",
        "    ''' Plot the feature importance calculated by calc_importances ''' \n",
        "    plt.figure()\n",
        "    # Set the style\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    # list of x locations for plotting\n",
        "    x_values = list(range(len(importances)))\n",
        "    # Make a bar chart\n",
        "    plt.barh(x_values, importances)\n",
        "    # Tick labels for x axis\n",
        "    plt.yticks(x_values, feature_list)\n",
        "    # Axis labels and title\n",
        "    plt.xlabel('Importance'); plt.ylabel('Variable'); plt.title('Variable Importances')\n",
        "    \n",
        "    \n",
        "plot_feat_importances(calc_importances(forest, feature_names),  feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF-W-c9SfA0l"
      },
      "source": [
        "## 3.3. Permutation importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-1tB1MBfCfL"
      },
      "source": [
        "Permutation importance shows which features cause the largest drop in skill if they are randomly shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkZYz1SvfJtX"
      },
      "outputs": [],
      "source": [
        "# Single-pass permutation\n",
        "permute = permutation_importance(\n",
        "    forest, X, y, n_repeats=20, random_state=fd[\"random_state\"])\n",
        "\n",
        "# Sort the importances\n",
        "sorted_idx = permute.importances_mean.argsort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "A0MfL96PfTwp",
        "outputId": "16934ee3-d891-4022-e67d-abd39ba6c924"
      },
      "outputs": [],
      "source": [
        "def plot_perm_importances(permute, sorted_idx, feature_list):\n",
        "    ''' Plot the permutation importances calculated in previous cell '''\n",
        "    # Sort the feature list based on \n",
        "    new_feature_list = []\n",
        "    for index in sorted_idx:  \n",
        "        new_feature_list.append(feature_list[index])\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.boxplot(permute.importances[sorted_idx].T,\n",
        "           vert=False, labels=new_feature_list)\n",
        "    ax.set_title(\"Permutation Importances\")\n",
        "    fig.tight_layout()\n",
        "    \n",
        "plot_perm_importances(permute, sorted_idx, feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "244YfBp9d7F5"
      },
      "source": [
        "# 4. Run random forest classifier on the testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raise Exception(\"you shall not pass.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuYsKqgJdpHB"
      },
      "source": [
        "Once you're happy with your model and feel ready, run your random forest on the testing data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "MFDon5m_ZQ2-",
        "outputId": "0410ad2b-68a7-44ae-e03b-ff47e09b6d11"
      },
      "outputs": [],
      "source": [
        "y_pred_test = forest.predict(X_test)\n",
        "\n",
        "acc = metrics.accuracy_score(y_test, y_pred_test)\n",
        "print(\"held-back testing accuracy: \", np.around(acc*100), '%')\n",
        "confusion_matrix(y_test, y_pred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkQ3eTWKk22V"
      },
      "source": [
        "üêß"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
