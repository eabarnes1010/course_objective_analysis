{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"YNYOWfvEbWzO"},"source":["# Using a random forest to estimate ozone air quality class given meteorological data\n","[![Latest release](https://badgen.net/github/release/Naereen/Strapdown.js)](https://github.com/eabarnes1010/ml_tutorial_csu/tree/main/code)\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eabarnes1010/course_objective_analysis/blob/main/code/random_forest_ozone_joshuatree.ipynb)\n","\n","* Modified for ATS 655 - Objective Analysis at Colorado State University led by Prof. Elizabeth Barnes\n","* Example by Daniel Hueholt and Zaibeth Carlo Frontera at Colorado State University (CSU) for Program for Climate Model Diagnosis and Intercomparison at Lawrence Livermore National Lab Machine Learning tutorial, adapted from notebook by Jamin Rader\n","* Data processed and Colab notebook created by TA Jamin Rader [CSU] for ATS 780A7 Spring 2022 at CSU led by Prof. Elizabeth Barnes\n","* Random forest code adapted from iris dataset example by Prof. Elizabeth Barnes, Aaron Hill and Wei-Ting Hsiao based on Avinash Navlani's Datacamp tutorial code"]},{"cell_type":"markdown","metadata":{"id":"8sVKVYstVW_p"},"source":["# 0. Set Up Environments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1651517331322,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"3ADJn5sDvJAU","outputId":"e44c7033-2ba9-4685-e52a-23b6a7b51495"},"outputs":[],"source":["try:\n","    import google.colab\n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False\n","print('IN_COLAB = ' + str(IN_COLAB))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7155,"status":"ok","timestamp":1651517338473,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"xWEMpofqvMuN","outputId":"8ee49a6b-6bad-4d11-c8f6-f77fd65df17e"},"outputs":[],"source":["import sys\n","import numpy as np\n","import seaborn as sb\n","\n","import pandas as pd\n","import datetime\n","import tensorflow as tf\n","#import tensorflow.keras as keras\n","import sklearn\n","#import pydot\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","# %matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1651517338474,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"uQ8qXrMovQFj","outputId":"f81a8448-49f8-4451-e634-17709a2e49ed"},"outputs":[],"source":["print(f\"python version = {sys.version}\")\n","print(f\"numpy version = {np.__version__}\")\n","print(f\"tensorflow version = {tf.__version__}\")  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-3QsgoIvSOr"},"outputs":[],"source":["# Uncomment this section if you want to save figures to a personal Google Drive\n","# if(IN_COLAB==True):\n","#     try:\n","#         from google.colab import drive\n","#         drive.mount('/content/drive', force_remount=True)\n","#         local_path = '/content/drive/My Drive/Colab Notebooks/'\n","#     except:\n","#         local_path = './'\n","# else:\n","#     local_path = '../figures/'"]},{"cell_type":"markdown","metadata":{"id":"xYzCs4hS0ShZ"},"source":["# 1. Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"Ts-S49oV0In4"},"source":["### 1.1 Data Overview"]},{"cell_type":"markdown","metadata":{"id":"sCosPTs30iFf"},"source":["This is ozone and meteorological data from [CASTNET](https://www.epa.gov/castnet) (Clean Air Status and Trends Network) for Joshua Tree National Park, located just outside of Palm Springs and about 100 miles east of Los Angeles. The National Park Service monitors ozone in their parks. Joshua Tree has recorded at least 30 exceedance days per year [since 2016](https://www.nps.gov/subjects/air/ozone-exceed.htm). An exceedance day occurs when the daily maximum 8-hour ozone average is 71 ppb or higher. For comparison, Rocky Mountain NP has only experienced 35 ozone exceedance days since 2016.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrDpNnArxE5V"},"outputs":[],"source":["# Read in data from url\n","url = \"https://raw.githubusercontent.com/eabarnes1010/course_ml_ats/main/data/ozone_data_joshuatreenp.csv\"\n","data = pd.read_csv(url,parse_dates=[\"DATE_TIME\"],infer_datetime_format=True)\n","\n","# Fix data issue with Daylight Savings Time\n","duplicate_dates = data['DATE_TIME'][data.duplicated('DATE_TIME')]\n","for dup_date in duplicate_dates:\n","  idx = data['DATE_TIME'].eq(dup_date).idxmax()\n","  data.at[idx, 'DATE_TIME'] = dup_date - pd.Timedelta(value=1, unit='hours')  \n","\n","# Add hour and day of year\n","data['HOUR'] = data['DATE_TIME'].dt.hour\n","data['MONTH'] = data['DATE_TIME'].dt.month\n","data['YEAR'] = data['DATE_TIME'].dt.year\n","data['DAYOFYEAR'] = data['DATE_TIME'].dt.dayofyear\n","data.sort_values('DATE_TIME', inplace=True, ignore_index = True)"]},{"cell_type":"markdown","metadata":{"id":"UfcWmi6-odBK"},"source":["Let's take a look at the data. We have data for ozone, temperature, relative humidity, and wind direction, among others."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1651517362492,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"a7gFEvuuCQo_","outputId":"72c13fb9-988b-4243-b814-777ce48514dd"},"outputs":[],"source":["display(data.head())"]},{"cell_type":"markdown","metadata":{"id":"fcts6nWEo1bZ"},"source":["### 1.2 Define Input and Output"]},{"cell_type":"markdown","metadata":{"id":"VCegAFcPrvKZ"},"source":["The 2015 benchmark for [human health ozone condition](https://www.nps.gov/articles/analysis-methods2020.htm) is shown here. Let us predict whether the ozone quality will be classified as good, fair, or poor over 8-hour periods.\n","\n","**Good**   $\\leq$ 54.9 ppb\n","\n","**Fair**   55.0 - 70.9 ppb\n","\n","**Poor**   $\\geq$ 71.0 ppb\n","\n","Let's start out by training our model using temperature, relative humidity, wind speed, and day of year."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1651517362493,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"zraOwSRgsf6f","outputId":"ce8d0997-15f5-48f0-9a0f-47bb216312f4"},"outputs":[],"source":["# Here are all the different variables that we could use for training our random \n","# forest (except ozone, of course!)\n","data.columns"]},{"cell_type":"markdown","metadata":{"id":"Rd5gJJUCsWjS"},"source":["**EDIT the Input Variables Here:** Reminder, if you choose to use wind direction, you must first convert it to a vector for averaging."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMoAfkADsSFo"},"outputs":[],"source":["# MODIFY: List of strings from the available column names in the data set\n","INPUT_VARIABLES = ['TEMPERATURE', 'RELATIVE_HUMIDITY', \n","                   'WINDSPEED',]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":429,"status":"ok","timestamp":1651517362909,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"u9LjpXXgC3zd","outputId":"dbcb8ce5-5e22-427f-9b44-2c6a5226bc74"},"outputs":[],"source":["# Let's isolate our variables of interest and take the 8-hour running mean\n","\n","# First using input and output variables together to take running mean\n","df_data_to_be_used = data[['OZONE'] + INPUT_VARIABLES] \n","\n","# Here we take the 8-hour rolling mean (note: DATE_TIME does not work)\n","df_data_to_be_used = df_data_to_be_used.rolling(8).mean()\n","\n","# Now adding Date and Time components\n","df_data_to_be_used[['DATE_TIME', 'HOUR', 'MONTH', 'YEAR']] = \\\n","    data[['DATE_TIME', 'HOUR', 'MONTH', 'YEAR']] \n","\n","# Dropping NaNs\n","df_data_to_be_used.dropna(inplace=True)\n","\n","display(df_data_to_be_used.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1651517362910,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"PcO80t7pJKgw","outputId":"68dce50a-8ddf-4646-aa30-8d7e1a71cab3"},"outputs":[],"source":["# Creating a numpy array for our inputs and outputs\n","input = df_data_to_be_used[INPUT_VARIABLES].values\n","output_raw = df_data_to_be_used['OZONE'].values\n","\n","# Creating numpy arrays for time/date info for visualizations\n","hour = df_data_to_be_used['HOUR'].values\n","month = df_data_to_be_used['MONTH'].values\n","year = df_data_to_be_used['YEAR'].values\n","\n","# Turning ozone into classification problem:\n","  # Class 0: Good, Class 1: Fair, Class 2: Poor\n","output_class = (output_raw >= 55).astype(int) + (output_raw >= 71).astype(int)\n","output = (output_class.reshape(-1,1) == np.unique(output_class)).astype(int)\n","\n","# Here is how our data is encoded into classes.\n","print('Ozone Value:', output_raw[0])\n","print('Ozone Class:', output_class[0])\n","print('Encoded As:', output[0])\n","print()\n","print('Ozone Value:', output_raw[2000])\n","print('Ozone Class:', output_class[2000])\n","print('Encoded As:', output[2000])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":869,"status":"ok","timestamp":1651517363771,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"LjiBWXuw2Qbo","outputId":"d260febd-3af6-4917-f9c0-84cad17318fd"},"outputs":[],"source":["# Printing the shapes of our input and output arrays (#samples , #dimension of input/output)\n","print('Input Array Shape:', input.shape)\n","print('Output Array Shape:', output.shape)"]},{"cell_type":"markdown","metadata":{"id":"7lrkzNsd3vPY"},"source":["### 1.3 Visualizing our Data"]},{"cell_type":"markdown","metadata":{"id":"saw7iTsVVQJp"},"source":["Let's look at what our output data actually looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1651517363773,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"tO_45vnhDeDq","outputId":"9fdc028f-9f9b-4a7e-80d7-c7210a7966a2"},"outputs":[],"source":["# How often does our data fall into each category?\n","calcpercent = lambda cat: str((np.sum(output_class == cat)/len(output_class)*100).astype(int))\n","\n","# Print out the sizes of each class\n","print('Frequency for each Ozone Category')\n","print('Good: ' + calcpercent(0) + '%')\n","print('Fair: ' + calcpercent(1) + '%')\n","print('Poor: ' + calcpercent(2) + '%')\n","\n","# Make bar plot of label distributions\n","# fig,ax = plt.subplots()\n","# label_names = ['Good', 'Fair', 'Poor']\n","# labels, counts = np.unique(output_class, return_counts=True)\n","# plt.bar(labels, (counts/np.sum(counts)*100), align='center', width=0.4, color='#8F00FF')\n","# plt.gca().set_xticks(labels)\n","# plt.xlim(-1,len(label_names))\n","# ax.set_xticklabels(label_names)\n","# plt.title('Sample percentage by label')\n","# plt.ylabel('Sample percentage')\n","# plt.yticks([0, 20, 40, 60])\n","# plt.savefig(local_path + '/' + 'classes'+'.png', dpi_val=400)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1651517363984,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"6AjbZz3Y3ufo","outputId":"80d9e630-9e17-488c-f25d-18716d3416b8"},"outputs":[],"source":["# Distribution of ozone concentrations\n","sb.displot(output_raw, kind='hist')\n","plt.xlabel('Ozone (ppb)')\n","plt.axvline(x=71, color='red')\n","plt.axvline(x=55, color='goldenrod')\n","plt.text(56, 50, 'Fair', rotation=90, color='goldenrod')\n","plt.text(72, 50, 'Poor', rotation=90, color='red')\n","\n","plt.title('Histogram of O3 Concentrations in Joshua Tree NP', fontweight='demi')\n","plt.plot()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":388},"executionInfo":{"elapsed":3554,"status":"ok","timestamp":1651517367528,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"Tp38o54I5ESG","outputId":"c2e1ca45-4876-4c6a-e0be-1ed08a804230"},"outputs":[],"source":["# Distribution of ozone concentrations in each month\n","\n","fig, axs = plt.subplots(2, 6, figsize = (15,5))\n","\n","for m in np.arange(12):\n","  ax = axs[m//6,m%6]\n","  sb.histplot(output_raw[month == m+1], ax=ax)\n","  ax.set_title(datetime.datetime.strptime(str(m+1), \"%m\").strftime(\"%b\"))\n","  ax.axvline(x=71, color='red')\n","  ax.axvline(x=55, color='goldenrod')\n","  ax.set_xlim(0,100)\n","  ax.set_ylim(0,500)\n","\n","fig.tight_layout(pad=1.0)\n","print('O3 Concentrations in Each Month')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":388},"executionInfo":{"elapsed":3651,"status":"ok","timestamp":1651517371173,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"vqdk4zfY-ACv","outputId":"236bf25f-6e1b-4edf-e7b4-77b5650014a7"},"outputs":[],"source":["# Distribution of ozone concentrations in each year\n","\n","fig, axs = plt.subplots(2, 7, figsize = (15,5))\n","\n","axidx = 0\n","for y in np.unique(year):\n","  ax = axs[axidx//7,axidx%7]\n","  sb.histplot(output_raw[year == y], ax=ax)\n","  ax.set_title(y)\n","  ax.axvline(x=71, color='red')\n","  ax.axvline(x=55, color='goldenrod')\n","  ax.set_xlim(0,100)\n","  axidx+=1\n","\n","fig.tight_layout(pad=1.0)\n","print('O3 Concentrations in Each Year')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1651517371486,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"O7KyKo2nCkMS","outputId":"9c3ac452-6fec-4183-fe11-1a7f93208607"},"outputs":[],"source":["# How many samples are available in each year? Data cannot be used if \n","# there are NaNs (see 2013)\n","sb.histplot(year)\n","plt.title('Number of Usable Samples in Each Year');"]},{"cell_type":"markdown","metadata":{"id":"84Z-QzHQ3Gos"},"source":["### 1.4 Partitioning Data in Training, Validation, and Testing Sets"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eZzjby-W137P"},"source":["Our data is highly autocorrelated, so we are going to separate training, validation, and testing by grabbing different years of data. *Not* by random sampling.\n","\n","**Some Variable Definitions**\n","\n","* ```Xtrain/Xval:*** 2-D Arrays of input data (shape: #samples, #input_variables)```\n","\n","* ```Ttrain/Tval:*** 2-D Arrays of target output data (true ozone class likelihood; shape: #samples, #classes)```\n","\n","* ```Ptrain/Pval:*** 2-D Arrays of predicted output data (predicted ozone class likelihoods; shape: #samples, #classes)```\n","\n","* ```Xtrain_raw/Xval_raw:*** 2-D Arrays of raw (pre-standardized) input data (shape: #samples, #input_variables)```\n","\n","* ```O3train/O3val:*** 1-D Arrays of raw ozone measurements (ppb; shape: #samples)```\n","\n","* ```Cttrain/Ctval:*** 1-D Arrays of the true ozone class (shape: #samples)```\n","\n","* ```Cptrain/Cpval:*** 1-D Arrays of the predicted ozone class with the highest likelihood (shape #samples)```"]},{"cell_type":"markdown","metadata":{"id":"bD_e5dU9xgA4"},"source":["**EDIT the years used for training, validation and testing here:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qS34wb-2xSUV"},"outputs":[],"source":["# Default: Use the years 2010-2017 for training, 2018-2019 for validation\n","TRAIN_RANGE = (2010, 2017)\n","VAL_RANGE = (2018, 2019)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1651517371489,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"rdaEo2W8Jfv4","outputId":"8a2f64cf-eaa8-4969-87cb-cf2591ca031e"},"outputs":[],"source":["# Splitting into training, testing, validation\n","\n","# This function returns a boolean array of years that fall within the given year range\n","year_bool = lambda yrrange: np.logical_and(year>=yrrange[0], year<=yrrange[1])\n","\n","# Create the input and output arrays from training, testing, validation sets\n","# Inputs haven't been standardized yet (thus \"_raw\")\n","\n","Xtrain_raw = input[year_bool(TRAIN_RANGE)] # these are the inputs (X)\n","Ttrain = output[year_bool(TRAIN_RANGE)] # these are the outputs (T is for target)\n","\n","Xval_raw = input[year_bool(VAL_RANGE)]\n","Tval = output[year_bool(VAL_RANGE)]\n","\n","# These are the raw outputs in each set for use later\n","O3train = output_raw[year_bool(TRAIN_RANGE)]\n","O3val = output_raw[year_bool(VAL_RANGE)]\n","\n","print('Shapes:')\n","print('  Xtrain: ', Xtrain_raw.shape)\n","print('  Xval: ', Xval_raw.shape)\n","print('  Ttrain: ', Ttrain.shape)\n","print('  Tval: ', Tval.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14W1NsXFWWLy"},"outputs":[],"source":["# Standardizing the training, testing, and validation data\n","\n","# This function takes a raw set of input fields (for example, the training, \n","# validation, or testing arrays), and standardizes it based on the training data.\n","\n","standardize_input = lambda dat, x, s: (dat - x)/s \n","\n","# Calculate mean and standard deviation of the training data\n","trainmean = Xtrain_raw.mean(axis=0) \n","trainstd  = Xtrain_raw.std(axis=0) \n","\n","Xtrain = standardize_input(Xtrain_raw, trainmean, trainstd)\n","Xval = standardize_input(Xval_raw, trainmean, trainstd)"]},{"cell_type":"markdown","metadata":{"id":"sI3_qEFUKkjb"},"source":["# 2. Random Forest"]},{"cell_type":"markdown","metadata":{"id":"ZWXnTtfnKxnG"},"source":["### 2.1 Building the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvMH7n2KJkK9"},"outputs":[],"source":["# Let's import some different things we will use to grow the random forest and \n","# evaluate or visualize its performance.\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Y79R5VYNMQL"},"outputs":[],"source":["### Here is where we actually build the model\n","\n","# Create and Train the Random Forest Classifier\n","#-------------------------------------------------------------------------------------------------\n","# MODIFY: important tunable parameters (hyperparameters) for model\n","number_of_trees = 1               # number of trees to \"average\" together to create a random forest\n","tree_depth = 1                     # maximum depth allowed for each tree\n","node_split = 30                     # minimum number of training samples needed to split a node\n","leaf_samples = 30                   # minimum number of training samples required to make a leaf node\n","criterion = 'gini'                 # 'gini' or 'entropy'\n","bootstrap = False                  # whether to perform \"bagging=bootstrap aggregating\" or not\n","max_samples = None                 # number of samples to grab when training each tree IF bootstrap=True, otherwise None\n","class_weight = None                # class weights, default None. Use 'balanced' to weight by class frequency, or customize with format [{0:1.0,1:1.0}, {0:1.0,1:1.0}, {0:1.0,1:1.0}]\n","RAND_STATE = 13                    # specify random state for reproducibility\n","#-------------------------------------------------------------------------------------------------\n","\n","rf=RandomForestClassifier(n_estimators=number_of_trees,\n","                           criterion=criterion,     \n","                           random_state=RAND_STATE,\n","                           min_samples_split = node_split,\n","                           min_samples_leaf = leaf_samples,\n","                           max_depth = tree_depth,\n","                           bootstrap = bootstrap,\n","                           class_weight = class_weight,\n","                           max_samples = max_samples,\n","                          )\n","\n"]},{"cell_type":"markdown","metadata":{"id":"13wObSJaoSsk"},"source":["### 2.2 Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1695,"status":"ok","timestamp":1651517373730,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"7QY2ycJWOy5B","outputId":"a6d9cf2a-417b-4837-f4f4-b7a2ab1693a5"},"outputs":[],"source":["# Train the model using the training sets\n","rf.fit(Xtrain,Ttrain)"]},{"cell_type":"markdown","metadata":{"id":"LIkR0MdnQgJR"},"source":["Now we can make predictions on the training and validation datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ry9E6bJpAkU"},"outputs":[],"source":["Ptrain=rf.predict(Xtrain) # make predictions on the training dataset\n","Pval=rf.predict(Xval) # make predictions on the validation set\n","\n","# Useful for evaluating performance later\n","Cptrain = Ptrain.argmax(axis=1) # 1-D array of predicted class (highest likelihood)\n","Cpval = Pval.argmax(axis=1)\n","Cttrain = Ttrain.argmax(axis=1) # 1-D array of truth class\n","Ctval = Tval.argmax(axis=1)"]},{"cell_type":"markdown","metadata":{"id":"fQT3clZbZDXn"},"source":["### 2.3 Model Performance\n"]},{"cell_type":"markdown","metadata":{"id":"nzF7BXPfQD-6"},"source":["Now we will evaluate the random forest's performance in more depth. Categorical accuracy describes how often any class was correctly predicted. However, categorical accuracy can be deceptive in problems with unbalanced classes! Since 65% of our data are from days with \"Good\" ozone, our model could predict \"Good\" every time and we would still be 65% accurate. This doesn't allow us to learn anything about what it takes to predict \"Fair\" or \"Poor\" ozone days. Below, we compare categorical accuracy to weighted categorical accuracy, which takes into account class imbalances."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1651517374198,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"F5mjzp1EjRPo","outputId":"0b051878-0caf-4374-cb2e-fb56f1ec4a10"},"outputs":[],"source":["from sklearn.metrics import f1_score, accuracy_score\n","print('Validation Categorical Accuracy:', accuracy_score(Ctval, Cpval) )\n","\n","# Weight equal to the inverse of the frequency of the class\n","cat_weights = np.sum((1 / np.mean(Ttrain, axis=0)) * Tval, axis=1) \n","print('** Validation Weighted Categorical Accuracy:', accuracy_score(Ctval, Cpval, sample_weight=cat_weights) )"]},{"cell_type":"markdown","metadata":{"id":"P2jc6i82Rv8D"},"source":["A useful way to visualize a model's performance on a classification problem is through a *confusion matrix*. This displays the frequency of the predicted class agains the frequency of the true (target) class. A confusion matrix makes it simple to see whether a certain class is being underpredicted or overpredicted in favor of another. In a perfect confusion matrix, the diagonal entries will be one (or 100% if expressed in percentage), with all other entries zero (i.e. it will be the identity matrix)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"capsBG1d1uW0"},"outputs":[],"source":["def confusion_matrix(predclasses, targclasses):\n","\n","  class_names = np.unique(targclasses)\n","\n","  table = []\n","  for pred_class in class_names:\n","    row = []\n","    for true_class in class_names:\n","        row.append(100 * np.mean(predclasses[targclasses == true_class] == pred_class))\n","    table.append(row)\n","  class_titles_t = [\"T(Good)\", \"T(Fair)\", \"T(Poor)\"]\n","  class_titles_p = [\"P(Good)\", \"P(Fair)\", \"P(Poor)\"]\n","  conf_matrix = pd.DataFrame(table, index=class_titles_p, columns=class_titles_t)\n","  display(conf_matrix.style.background_gradient(cmap='Greens').format(\"{:.1f}\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"elapsed":147,"status":"ok","timestamp":1651517374340,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"8JZegNvJ1x6-","outputId":"78573f16-3253-49ed-ba6d-7e094950c3ec"},"outputs":[],"source":["print(\"Predicted versus Target Classes\")\n","confusion_matrix(Cptrain, Cttrain)\n","confusion_matrix(Cpval, Ctval)"]},{"cell_type":"markdown","metadata":{"id":"U7xrYtGlU0NR"},"source":["Violin plots of the class frequencies can tell us whether the distribution of the predicted classes match the actual values."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1651517374676,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"oLyRWUOu4sMN","outputId":"cf115eb2-8b26-4c9b-d5c9-a05011768009"},"outputs":[],"source":["df_class0 = pd.DataFrame(O3val[Cpval == 0])\n","df_class1 = pd.DataFrame(O3val[Cpval == 1])\n","df_class2 = pd.DataFrame(O3val[Cpval == 2])\n","\n","oi_pal = {0: '#56B4E9', 1: '#E69F00', 2: '#D55E00'}\n","sb.violinplot(data = [O3val[Cpval == 0], \n","                     O3val[Cpval == 1], \n","                     O3val[Cpval == 2]], palette=oi_pal)\n","\n","plt.axhline(55, color='goldenrod', zorder=0)\n","plt.axhline(71, color='red', zorder=0)\n","plt.ylabel('Ozone (ppb)')\n","plt.xlabel('Predicted Class')\n","plt.xticks([0,1,2], [\"Good\", \"Fair\", \"Poor\"],)\n","plt.title('O3 Concentrations when each Class is Predicted', fontweight='demi')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xlQ7lQs6qMab"},"source":["# 3. Explaining the model's predictions\n","We have determined whether the random forest's output is accurate. We can now turn to exploring how these predictions came about!"]},{"cell_type":"markdown","metadata":{"id":"Fjh2w71OEblQ"},"source":["### 3.1 Visualize an individual tree\n","Decision trees are *interpretable*, i.e. their decisions can be directly understood. Looking at individual trees can help us to get a sense of whether the forest model is making physically reasonable predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":698},"executionInfo":{"elapsed":280,"status":"ok","timestamp":1651517374953,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"n1S0Faf5rf1U","outputId":"5ee3392a-8ca7-4361-9d99-e658fb5b174b"},"outputs":[],"source":["from sklearn.tree import export_graphviz\n","from graphviz import Source\n","tree = rf[-1]\n","\n","Source(export_graphviz(tree,\n","                        out_file=None,\n","                        filled=True,\n","                        proportion=False,\n","                        leaves_parallel=False,\n","                        class_names=['good', 'poor', 'fair'],\n","                        feature_names=INPUT_VARIABLES))"]},{"cell_type":"markdown","metadata":{"id":"W4M3iypZSKIJ"},"source":["Plotting feature importance reveals which features were most critical to the forest's predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413},"executionInfo":{"elapsed":144,"status":"ok","timestamp":1651517375095,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"IG0_Gn7nRH2O","outputId":"3158aff9-8ac7-4443-c998-20bb5d4916e7"},"outputs":[],"source":["def calc_importances(rf, feature_list):\n","\n","    # Get numerical feature importances\n","    importances = list(rf.feature_importances_)\n","\n","    # List of tuples with variable and importance\n","    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n","\n","    # Sort the feature importances by most important first\n","    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n","\n","    # Print out the feature and importances \n","    print('')\n","    [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n","    print('')\n","\n","    return importances\n","\n","def plot_feat_importances(importances, feature_list): \n","    plt.figure()\n","    # Set the style\n","    plt.style.use('fivethirtyeight')\n","    # list of x locations for plotting\n","    x_values = list(range(len(importances)))\n","    # Make a bar chart\n","    plt.barh(x_values, importances)\n","    # Tick labels for x axis\n","    plt.yticks(x_values, feature_list)\n","    # Axis labels and title\n","    plt.xlabel('Importance'); plt.ylabel('Variable'); plt.title('Variable Importances')\n","    \n","    \n","plot_feat_importances(calc_importances(rf,  INPUT_VARIABLES),  INPUT_VARIABLES)\n"]},{"cell_type":"markdown","metadata":{"id":"obAnAa_dUqYP"},"source":["Permutation importance shows which features cause the largest drop in skill if they are randomly shuffled. Note that this calculation can be slow."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVwSbzyvVbSZ"},"outputs":[],"source":["# from sklearn.inspection import permutation_importance\n","# # Single-pass permutation\n","# permute = permutation_importance(rf, Xval, Tval, n_repeats=20, \n","#                                  random_state=RAND_STATE)\n","\n","# # Sort the importances\n","# sorted_idx = permute.importances_mean.argsort()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1651517384068,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"0Fwr2hzoVibA","outputId":"4eed5e1d-9c31-44ec-d816-607b310d5574"},"outputs":[],"source":["# def plot_perm_importances(permute, sorted_idx, feature_list):\n","#     # Plot the permutation importances\n","    \n","#     new_feature_list = []\n","#     for index in sorted_idx:  \n","#         new_feature_list.append(feature_list[index])\n","\n","#     fig, ax = plt.subplots()\n","#     ax.boxplot(permute.importances[sorted_idx].T,\n","#            vert=False, labels=new_feature_list)\n","#     ax.set_title(\"Permutation Importances\")\n","#     fig.tight_layout()\n","    \n","# plot_perm_importances(permute, sorted_idx, INPUT_VARIABLES)"]},{"cell_type":"markdown","metadata":{"id":"q7reR-uCQM7E","tags":[]},"source":["# 4. Model Testing: Do Not Run Until You've Tuned Your Model!"]},{"cell_type":"markdown","metadata":{"id":"QqIVAjqyQUih","jupyter":{"source_hidden":true},"tags":[]},"source":["We have set aside some ozone data to be used for testing. Tune your model on the VALIDATION data to the best of your abilities. Once you have a model that you are confident in, run it on the testing data to see how it performs on data it has never seen before. This section uses Weighted Categorical Accuracy to measure model performance. *Once you have run the model on the testing data, you cannot continue to tune the modelâ€“it is \"cheating\" to tune the model to data intended to verify its performance! Once you have used the testing data, your project has finished!*"]},{"cell_type":"markdown","metadata":{"id":"mmp7Ld72X6ft"},"source":[" **Enter the given CODE to run the testing data.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGEseP36QTL8"},"outputs":[],"source":["CODE = ''"]},{"cell_type":"markdown","metadata":{"id":"Vp3-WphNX8UZ"},"source":["DO NOT EDIT THE FOLLOWING:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQuFhq_lRzNB"},"outputs":[],"source":["def compete():\n","\n","  # Read in data from url\n","  url = \"https://raw.githubusercontent.com/eabarnes1010/course_ml_ats/main/data/ozone_data_joshuatreenp_\" + CODE + \".csv\"\n","  data = pd.read_csv(url,parse_dates=[\"DATE_TIME\"],infer_datetime_format=True)\n","\n","  # Fix data issue with Daylight Savings Time\n","  duplicate_dates = data['DATE_TIME'][data.duplicated('DATE_TIME')]\n","  for dup_date in duplicate_dates:\n","    idx = data['DATE_TIME'].eq(dup_date).idxmax()\n","    data.at[idx, 'DATE_TIME'] = dup_date - pd.Timedelta(value=1, unit='hours')  \n","\n","  # Add hour and day of year\n","  data['HOUR'] = data['DATE_TIME'].dt.hour\n","  data['MONTH'] = data['DATE_TIME'].dt.month\n","  data['YEAR'] = data['DATE_TIME'].dt.year\n","  data['DAYOFYEAR'] = data['DATE_TIME'].dt.dayofyear\n","  data.sort_values('DATE_TIME', inplace=True, ignore_index = True)\n","\n","  df_data_to_be_used = data[['OZONE'] + INPUT_VARIABLES] \n","  df_data_to_be_used = df_data_to_be_used.rolling(8).mean()\n","  df_data_to_be_used[['DATE_TIME', 'HOUR', 'MONTH', 'YEAR']] = \\\n","      data[['DATE_TIME', 'HOUR', 'MONTH', 'YEAR']] \n","  df_data_to_be_used.dropna(inplace=True)\n","\n","  Xcompete_raw = df_data_to_be_used[INPUT_VARIABLES].values\n","  output_raw = df_data_to_be_used['OZONE'].values\n","  hour = df_data_to_be_used['HOUR'].values\n","  month = df_data_to_be_used['MONTH'].values\n","  year = df_data_to_be_used['YEAR'].values\n","\n","  output_class = (output_raw >= 55).astype(int) + (output_raw >= 71).astype(int)\n","  Tcompete = (output_class.reshape(-1,1) == np.unique(output_class)).astype(int)\n","  year_bool = lambda yrrange: np.logical_and(year>=yrrange[0], year<=yrrange[1])\n","  \n","  standardize_input = lambda dat, x, s: (dat - x)/s \n","  Xcompete = standardize_input(Xcompete_raw, trainmean, trainstd)\n","\n","  Pcompete = rf.predict(Xcompete)\n","  Cpcompete = Pcompete.argmax(axis=1)\n","  Ctcompete = Tcompete.argmax(axis=1)\n","\n","  cat_weights = np.sum((1 / np.mean(Ttrain, axis=0)) * Tcompete, axis=1) \n","  print('Congrats! Your overall weighted categorical accuracy is:', accuracy_score(Ctcompete, Cpcompete, sample_weight=cat_weights) )\n","  print(\"Predicted versus Target Classes for Competition Data\")\n","  confusion_matrix(Cpcompete, Ctcompete)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"elapsed":745,"status":"error","timestamp":1651517384808,"user":{"displayName":"Daniel Hueholt","userId":"14834145460968199011"},"user_tz":360},"id":"eMy7TLscU82g","outputId":"fdb0c64a-e830-4c01-e2e0-369fa15bbb7e"},"outputs":[],"source":["compete()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0NUbJhqVAz-"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"rf_ozone_joshuatree.ipynb","provenance":[{"file_id":"https://github.com/eabarnes1010/course_ml_ats/blob/main/code/ann_ozone_joshuatree.ipynb","timestamp":1650913461922},{"file_id":"1GtH4FaaaqRewMY8KlM5DOhHQl1ZPdiWt","timestamp":1642113733706}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
